=============== Experiment Setup ===============
data:
  downsample: true
  normalize: false
  ntest_frac: 0.5
  ntrain_frac: 0.5
  secs_per_fn: 0.1
  x_test_fp: datasets/blues_driver/clean_inmyarmsriff.mp3
  x_train_fp: datasets/blues_driver/clean_inmyarmsriff.mp3
  y_test_fp: datasets/blues_driver/dirty_inmyarmsriff.mp3
  y_train_fp: datasets/blues_driver/dirty_inmyarmsriff.mp3
dataset: blues_driver
logging:
  display: false
model:
  channel_lift: true
  channel_lift_size: 128
  conv: true
  dense_init: he_normal
  integration_kernels: Gaussian,Gaussian,
  interpolation_kernel: Gaussian
  kernel_init: constant
  non_linearity: gelu
  num_output_layers: 1
  num_quad_pts: 10
  quad_type: hermite
  residual_block: true
  width: 128
optim:
  end_value: 1.0e-05
  gamma: 0.85
  init_value: 1.0e-05
  num_cycles: 6
  optimizer: Adam
  peak_value: 3.0e-05
  warmup_frac: 0.3
seed: 42
testing:
  batch_size: 20
  eval_at: 10000000
training:
  batch_size: 20
  epochs: 30000
  log_at: 20
  loss_fn: l2_relative_error
  shuffle: true

================================================
loading data...
Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
DEVICE = gpu
created datasets, train.x.shape=(74, 4410), train.y.shape=(74, 4410)
step 0, train_loss: 1.0030934810638428
step 0, test_loss: 1.0024474891456399
step 20, train_loss: 0.9996889233589172
step 40, train_loss: 0.9994275569915771
step 60, train_loss: 0.9991686344146729
step 80, train_loss: 0.998855471611023
step 100, train_loss: 0.998450756072998
step 120, train_loss: 0.997935950756073
step 140, train_loss: 0.9975903034210205
step 160, train_loss: 0.9969899654388428
step 180, train_loss: 0.9964648485183716
step 200, train_loss: 0.995893120765686
step 220, train_loss: 0.9953083395957947
step 240, train_loss: 0.9946825504302979
step 260, train_loss: 0.9945731163024902
step 280, train_loss: 0.9940317869186401
step 300, train_loss: 0.993762195110321
step 320, train_loss: 0.9932732582092285
step 340, train_loss: 0.9929558634757996
step 360, train_loss: 0.9926528334617615
step 380, train_loss: 0.9921107292175293
step 400, train_loss: 0.9920693039894104
step 420, train_loss: 0.9921558499336243
step 440, train_loss: 0.9921112060546875
step 460, train_loss: 0.9917852878570557
step 480, train_loss: 0.991807222366333
step 500, train_loss: 0.9921371936798096
step 520, train_loss: 0.9913405179977417
step 540, train_loss: 0.9918837547302246
step 560, train_loss: 0.9916197657585144
step 580, train_loss: 0.9916496276855469
step 600, train_loss: 0.9917047619819641
step 620, train_loss: 0.991523265838623
step 640, train_loss: 0.9917958974838257
step 660, train_loss: 0.9916211366653442
step 680, train_loss: 0.9918646216392517
step 700, train_loss: 0.9916062355041504
step 720, train_loss: 0.9918431639671326
step 740, train_loss: 0.991736888885498
step 760, train_loss: 0.9913415908813477
step 780, train_loss: 0.9911630153656006
step 800, train_loss: 0.9912413954734802
step 820, train_loss: 0.9914777278900146
step 840, train_loss: 0.991188108921051
step 860, train_loss: 0.991155743598938
step 880, train_loss: 0.9915983080863953
step 900, train_loss: 0.9914380311965942
step 920, train_loss: 0.99208664894104
step 940, train_loss: 0.9912911653518677
step 960, train_loss: 0.9913674592971802
step 980, train_loss: 0.9914506673812866
step 1000, train_loss: 0.9911795854568481
step 1020, train_loss: 0.9918766021728516
step 1040, train_loss: 0.9914595484733582
step 1060, train_loss: 0.9919126033782959
step 1080, train_loss: 0.9908419847488403
step 1100, train_loss: 0.9912147521972656
step 1120, train_loss: 0.9908944368362427
step 1140, train_loss: 0.9905315637588501
step 1160, train_loss: 0.990180492401123
step 1180, train_loss: 0.989648699760437
step 1200, train_loss: 0.988781750202179
step 1220, train_loss: 0.9880549907684326
step 1240, train_loss: 0.9869050979614258
step 1260, train_loss: 0.9865227937698364
step 1280, train_loss: 0.9841621518135071
step 1300, train_loss: 0.9834697842597961
step 1320, train_loss: 0.9832181930541992
step 1340, train_loss: 0.9815449714660645
step 1360, train_loss: 0.9804616570472717
step 1380, train_loss: 0.9801143407821655
step 1400, train_loss: 0.9798811674118042
step 1420, train_loss: 0.9794293642044067
step 1440, train_loss: 0.9786924123764038
step 1460, train_loss: 0.97939133644104
step 1480, train_loss: 0.9783738255500793
step 1500, train_loss: 0.978547990322113
step 1520, train_loss: 0.9802473187446594
step 1540, train_loss: 0.9781434535980225
step 1560, train_loss: 0.9785611033439636
step 1580, train_loss: 0.9774981737136841
step 1600, train_loss: 0.9768567085266113
step 1620, train_loss: 0.9780645370483398
step 1640, train_loss: 0.9777930974960327
step 1660, train_loss: 0.9775739908218384
step 1680, train_loss: 0.9782460331916809
step 1700, train_loss: 0.9780664443969727
step 1720, train_loss: 0.9776749014854431
step 1740, train_loss: 0.9771621227264404
step 1760, train_loss: 0.9778224229812622
step 1780, train_loss: 0.9772388339042664
step 1800, train_loss: 0.9772425293922424
step 1820, train_loss: 0.9777305126190186
step 1840, train_loss: 0.9779039621353149
step 1860, train_loss: 0.9759277105331421
step 1880, train_loss: 0.9774706363677979
step 1900, train_loss: 0.9779433012008667
step 1920, train_loss: 0.9773722887039185
step 1940, train_loss: 0.9766780138015747
step 1960, train_loss: 0.9759488701820374
step 1980, train_loss: 0.9771825075149536
step 2000, train_loss: 0.976516604423523
step 2020, train_loss: 0.9751272201538086
step 2040, train_loss: 0.976334273815155
step 2060, train_loss: 0.9774332642555237
step 2080, train_loss: 0.9771584868431091
step 2100, train_loss: 0.9759506583213806
step 2120, train_loss: 0.9761500358581543
step 2140, train_loss: 0.9754287004470825
step 2160, train_loss: 0.9763287305831909
step 2180, train_loss: 0.9754893779754639
step 2200, train_loss: 0.9754067063331604
step 2220, train_loss: 0.9753872156143188
step 2240, train_loss: 0.976311445236206
step 2260, train_loss: 0.9762009382247925
step 2280, train_loss: 0.9753427505493164
step 2300, train_loss: 0.9753950238227844
step 2320, train_loss: 0.9756008386611938
step 2340, train_loss: 0.9752418994903564
step 2360, train_loss: 0.975795328617096
step 2380, train_loss: 0.9754291772842407
step 2400, train_loss: 0.9742063283920288
step 2420, train_loss: 0.9738075733184814
step 2440, train_loss: 0.974823534488678
step 2460, train_loss: 0.9749735593795776
step 2480, train_loss: 0.9748525619506836
step 2500, train_loss: 0.9750902652740479
step 2520, train_loss: 0.9739292860031128
step 2540, train_loss: 0.9750312566757202
step 2560, train_loss: 0.9751688838005066
step 2580, train_loss: 0.974698007106781
step 2600, train_loss: 0.9742164015769958
step 2620, train_loss: 0.9747860431671143
step 2640, train_loss: 0.9751487970352173
step 2660, train_loss: 0.9750187993049622
step 2680, train_loss: 0.9746071696281433
step 2700, train_loss: 0.9740579724311829
step 2720, train_loss: 0.9750576019287109
step 2740, train_loss: 0.9745817184448242
step 2760, train_loss: 0.9744472503662109
step 2780, train_loss: 0.9748562574386597
step 2800, train_loss: 0.9745755791664124
step 2820, train_loss: 0.9736074805259705
step 2840, train_loss: 0.974175751209259
step 2860, train_loss: 0.9746807217597961
step 2880, train_loss: 0.9748339653015137
step 2900, train_loss: 0.9735105037689209
step 2920, train_loss: 0.9743078351020813
step 2940, train_loss: 0.9738610982894897
step 2960, train_loss: 0.9727399349212646
step 2980, train_loss: 0.9726963043212891
step 3000, train_loss: 0.973476767539978
step 3020, train_loss: 0.9731251001358032
step 3040, train_loss: 0.973004162311554
step 3060, train_loss: 0.9740656018257141
step 3080, train_loss: 0.9734548330307007
step 3100, train_loss: 0.9732980132102966
step 3120, train_loss: 0.9733719825744629
step 3140, train_loss: 0.9740442037582397
step 3160, train_loss: 0.9740660786628723
step 3180, train_loss: 0.9732455611228943
step 3200, train_loss: 0.9736405611038208
step 3220, train_loss: 0.973233699798584
step 3240, train_loss: 0.973180890083313
step 3260, train_loss: 0.9735620021820068
step 3280, train_loss: 0.9730998873710632
step 3300, train_loss: 0.9735987186431885
step 3320, train_loss: 0.974255383014679
step 3340, train_loss: 0.9740182161331177
step 3360, train_loss: 0.9740304946899414
step 3380, train_loss: 0.9730637669563293
step 3400, train_loss: 0.9732756614685059
step 3420, train_loss: 0.9732624888420105
step 3440, train_loss: 0.9731327295303345
step 3460, train_loss: 0.9738438129425049
step 3480, train_loss: 0.9725418090820312
step 3500, train_loss: 0.9719192385673523
step 3520, train_loss: 0.9735094308853149
step 3540, train_loss: 0.9737613201141357
step 3560, train_loss: 0.9736844301223755
step 3580, train_loss: 0.9735430479049683
step 3600, train_loss: 0.9728861451148987
step 3620, train_loss: 0.9730322360992432
step 3640, train_loss: 0.9736661911010742
step 3660, train_loss: 0.9732863903045654
step 3680, train_loss: 0.9737950563430786
step 3700, train_loss: 0.9735057353973389
step 3720, train_loss: 0.9725084900856018
step 3740, train_loss: 0.9727174639701843
step 3760, train_loss: 0.973230242729187
step 3780, train_loss: 0.9738819003105164
step 3800, train_loss: 0.9736753702163696
step 3820, train_loss: 0.9728860855102539
step 3840, train_loss: 0.9724619388580322
step 3860, train_loss: 0.9728870391845703
step 3880, train_loss: 0.9732953310012817
step 3900, train_loss: 0.9730392694473267
step 3920, train_loss: 0.9725944995880127
step 3940, train_loss: 0.973870038986206
step 3960, train_loss: 0.974082350730896
step 3980, train_loss: 0.9734337329864502
step 4000, train_loss: 0.971921443939209
step 4020, train_loss: 0.9739338159561157
step 4040, train_loss: 0.9737799167633057
step 4060, train_loss: 0.9737207889556885
step 4080, train_loss: 0.9739923477172852
step 4100, train_loss: 0.9734537601470947
step 4120, train_loss: 0.9724502563476562
step 4140, train_loss: 0.973490297794342
step 4160, train_loss: 0.9732205867767334
step 4180, train_loss: 0.9741432666778564
step 4200, train_loss: 0.9733933806419373
step 4220, train_loss: 0.9725731611251831
step 4240, train_loss: 0.9738327264785767
step 4260, train_loss: 0.973423421382904
step 4280, train_loss: 0.9731605052947998
step 4300, train_loss: 0.9726471304893494
step 4320, train_loss: 0.9730490446090698
step 4340, train_loss: 0.9729620814323425
step 4360, train_loss: 0.9718419313430786
step 4380, train_loss: 0.9734276533126831
step 4400, train_loss: 0.9737980961799622
step 4420, train_loss: 0.9725635051727295
step 4440, train_loss: 0.9734621047973633
step 4460, train_loss: 0.9735206961631775
step 4480, train_loss: 0.9730752110481262
step 4500, train_loss: 0.9730167388916016
step 4520, train_loss: 0.9731762409210205
step 4540, train_loss: 0.9723782539367676
step 4560, train_loss: 0.9726588129997253
step 4580, train_loss: 0.9727277159690857
step 4600, train_loss: 0.9727464318275452
step 4620, train_loss: 0.9725597500801086
step 4640, train_loss: 0.973325252532959
step 4660, train_loss: 0.9740092158317566
step 4680, train_loss: 0.9730079770088196
step 4700, train_loss: 0.9732602834701538
step 4720, train_loss: 0.9741559028625488
step 4740, train_loss: 0.9715788960456848
step 4760, train_loss: 0.9718281030654907
step 4780, train_loss: 0.9733269214630127
step 4800, train_loss: 0.9729505777359009
step 4820, train_loss: 0.9731219410896301
step 4840, train_loss: 0.9729770421981812
step 4860, train_loss: 0.9737513661384583
step 4880, train_loss: 0.9717170000076294
step 4900, train_loss: 0.9721997380256653
step 4920, train_loss: 0.9732332825660706
step 4940, train_loss: 0.9729387760162354
step 4960, train_loss: 0.9727976322174072
step 4980, train_loss: 0.9724300503730774
step 5000, train_loss: 0.9733082056045532
step 5020, train_loss: 0.9726366400718689
step 5040, train_loss: 0.9720015525817871
step 5060, train_loss: 0.9711736440658569
step 5080, train_loss: 0.972293496131897
step 5100, train_loss: 0.9715408086776733
step 5120, train_loss: 0.9723548889160156
step 5140, train_loss: 0.972383439540863
step 5160, train_loss: 0.9724817276000977
step 5180, train_loss: 0.9717617630958557
step 5200, train_loss: 0.9717515707015991
step 5220, train_loss: 0.97226482629776
step 5240, train_loss: 0.9718825817108154
step 5260, train_loss: 0.972835898399353
step 5280, train_loss: 0.9722375869750977
step 5300, train_loss: 0.9720409512519836
step 5320, train_loss: 0.9730974435806274
step 5340, train_loss: 0.9726839065551758
step 5360, train_loss: 0.9719378352165222
step 5380, train_loss: 0.9724646806716919
step 5400, train_loss: 0.9728572368621826
step 5420, train_loss: 0.9727051258087158
step 5440, train_loss: 0.9728017449378967
step 5460, train_loss: 0.9723193645477295
step 5480, train_loss: 0.9721384644508362
step 5500, train_loss: 0.9724529385566711
step 5520, train_loss: 0.972166895866394
step 5540, train_loss: 0.9730373620986938
step 5560, train_loss: 0.9727116823196411
step 5580, train_loss: 0.9725707173347473
step 5600, train_loss: 0.9727534055709839
step 5620, train_loss: 0.9727470874786377
step 5640, train_loss: 0.9734022617340088
step 5660, train_loss: 0.9727722406387329
step 5680, train_loss: 0.9715783596038818
step 5700, train_loss: 0.9712467789649963
step 5720, train_loss: 0.9720447063446045
step 5740, train_loss: 0.9742506742477417
step 5760, train_loss: 0.9727058410644531
step 5780, train_loss: 0.9722865223884583
step 5800, train_loss: 0.9729024171829224
step 5820, train_loss: 0.972793459892273
step 5840, train_loss: 0.9724540114402771
step 5860, train_loss: 0.9726759195327759
step 5880, train_loss: 0.9724482297897339
step 5900, train_loss: 0.9729933142662048
step 5920, train_loss: 0.9730738997459412
step 5940, train_loss: 0.9720804691314697
step 5960, train_loss: 0.9714276790618896
step 5980, train_loss: 0.9705872535705566
step 6000, train_loss: 0.9725598692893982
step 6020, train_loss: 0.971717357635498
step 6040, train_loss: 0.9712584018707275
step 6060, train_loss: 0.9726175665855408
step 6080, train_loss: 0.972427248954773
step 6100, train_loss: 0.9714473485946655
step 6120, train_loss: 0.9719857573509216
step 6140, train_loss: 0.9718014001846313
step 6160, train_loss: 0.972718358039856
step 6180, train_loss: 0.972419261932373
step 6200, train_loss: 0.9722775220870972
step 6220, train_loss: 0.9704422354698181
step 6240, train_loss: 0.9707571268081665
step 6260, train_loss: 0.9704937934875488
step 6280, train_loss: 0.9708299040794373
step 6300, train_loss: 0.9703219532966614
step 6320, train_loss: 0.9704810380935669
step 6340, train_loss: 0.9708646535873413
step 6360, train_loss: 0.9702443480491638
step 6380, train_loss: 0.9709115028381348
step 6400, train_loss: 0.9703700542449951
step 6420, train_loss: 0.9706687927246094
step 6440, train_loss: 0.9697204828262329
step 6460, train_loss: 0.9710696339607239
step 6480, train_loss: 0.970117449760437
step 6500, train_loss: 0.9706974029541016
step 6520, train_loss: 0.970924973487854
step 6540, train_loss: 0.9707937240600586
step 6560, train_loss: 0.9694187641143799
step 6580, train_loss: 0.9695615768432617
step 6600, train_loss: 0.9685173630714417
step 6620, train_loss: 0.9684975743293762
step 6640, train_loss: 0.9684743881225586
step 6660, train_loss: 0.9693014621734619
step 6680, train_loss: 0.9691616296768188
step 6700, train_loss: 0.9682520031929016
step 6720, train_loss: 0.9670121669769287
step 6740, train_loss: 0.9689267873764038
step 6760, train_loss: 0.9652959108352661
step 6780, train_loss: 0.9675118923187256
step 6800, train_loss: 0.965801477432251
step 6820, train_loss: 0.9666545391082764
step 6840, train_loss: 0.9667387008666992
step 6860, train_loss: 0.9665836095809937
step 6880, train_loss: 0.966205358505249
step 6900, train_loss: 0.9649323225021362
step 6920, train_loss: 0.9645015001296997
step 6940, train_loss: 0.9651777744293213
step 6960, train_loss: 0.9653456211090088
step 6980, train_loss: 0.9646507501602173
step 7000, train_loss: 0.9651689529418945
step 7020, train_loss: 0.9656305909156799
step 7040, train_loss: 0.9644026160240173
step 7060, train_loss: 0.9644545316696167
step 7080, train_loss: 0.9647302627563477
step 7100, train_loss: 0.9645991325378418
step 7120, train_loss: 0.9632238149642944
step 7140, train_loss: 0.9642062187194824
step 7160, train_loss: 0.9638368487358093
step 7180, train_loss: 0.9653091430664062
step 7200, train_loss: 0.9640808701515198
step 7220, train_loss: 0.9636980295181274
step 7240, train_loss: 0.9639361500740051
step 7260, train_loss: 0.9638768434524536
step 7280, train_loss: 0.9628666639328003
step 7300, train_loss: 0.9623841047286987
step 7320, train_loss: 0.9622905850410461
step 7340, train_loss: 0.9644057750701904
step 7360, train_loss: 0.9627626538276672
step 7380, train_loss: 0.9616588950157166
step 7400, train_loss: 0.9613158106803894
step 7420, train_loss: 0.9625962972640991
step 7440, train_loss: 0.9631091356277466
step 7460, train_loss: 0.9608467817306519
step 7480, train_loss: 0.9631301164627075
step 7500, train_loss: 0.9616297483444214
step 7520, train_loss: 0.961834192276001
step 7540, train_loss: 0.9609880447387695
step 7560, train_loss: 0.9622918367385864
step 7580, train_loss: 0.9608427882194519
step 7600, train_loss: 0.9605010151863098
step 7620, train_loss: 0.9614825248718262
step 7640, train_loss: 0.9606459140777588
step 7660, train_loss: 0.9613875150680542
step 7680, train_loss: 0.9604772329330444
step 7700, train_loss: 0.9619671106338501
step 7720, train_loss: 0.9611307978630066
step 7740, train_loss: 0.9603573083877563
step 7760, train_loss: 0.9619755744934082
step 7780, train_loss: 0.9611631631851196
step 7800, train_loss: 0.9604753851890564
step 7820, train_loss: 0.9598802328109741
step 7840, train_loss: 0.9597971439361572
step 7860, train_loss: 0.9602994918823242
step 7880, train_loss: 0.960946798324585
step 7900, train_loss: 0.9606305360794067
step 7920, train_loss: 0.9586185812950134
step 7940, train_loss: 0.9581357836723328
step 7960, train_loss: 0.9610793590545654
step 7980, train_loss: 0.9590833783149719
step 8000, train_loss: 0.9587767124176025
step 8020, train_loss: 0.960131049156189
step 8040, train_loss: 0.9589783549308777
step 8060, train_loss: 0.9581032991409302
step 8080, train_loss: 0.9596794247627258
step 8100, train_loss: 0.9607489705085754
step 8120, train_loss: 0.9585890173912048
step 8140, train_loss: 0.9572280049324036
step 8160, train_loss: 0.958061695098877
step 8180, train_loss: 0.9583578705787659
step 8200, train_loss: 0.9592846632003784
step 8220, train_loss: 0.9593759775161743
step 8240, train_loss: 0.9588685035705566
step 8260, train_loss: 0.9560534954071045
step 8280, train_loss: 0.9577975273132324
step 8300, train_loss: 0.9566441178321838
step 8320, train_loss: 0.9582288861274719
step 8340, train_loss: 0.9581373333930969
step 8360, train_loss: 0.9590228796005249
step 8380, train_loss: 0.9561035633087158
step 8400, train_loss: 0.9582366943359375
step 8420, train_loss: 0.9570112228393555
step 8440, train_loss: 0.9572216868400574
step 8460, train_loss: 0.9581424593925476
step 8480, train_loss: 0.9572216868400574
step 8500, train_loss: 0.9557212591171265
step 8520, train_loss: 0.9582654237747192
step 8540, train_loss: 0.955260694026947
step 8560, train_loss: 0.9581876397132874
step 8580, train_loss: 0.9568010568618774
step 8600, train_loss: 0.9572485089302063
step 8620, train_loss: 0.9561759829521179
step 8640, train_loss: 0.9560708403587341
step 8660, train_loss: 0.9560660123825073
step 8680, train_loss: 0.9567835330963135
step 8700, train_loss: 0.9548676013946533
step 8720, train_loss: 0.9567703008651733
step 8740, train_loss: 0.9549893736839294
step 8760, train_loss: 0.9561455249786377
step 8780, train_loss: 0.9569224119186401
step 8800, train_loss: 0.9572299122810364
step 8820, train_loss: 0.95517498254776
step 8840, train_loss: 0.9551635980606079
step 8860, train_loss: 0.9543944597244263
step 8880, train_loss: 0.9554144144058228
step 8900, train_loss: 0.9566154479980469
step 8920, train_loss: 0.9560548067092896
step 8940, train_loss: 0.9563521146774292
step 8960, train_loss: 0.9550439119338989
step 8980, train_loss: 0.9561722278594971
step 9000, train_loss: 0.9553492069244385
step 9020, train_loss: 0.9549374580383301
step 9040, train_loss: 0.954444944858551
step 9060, train_loss: 0.9547349214553833
step 9080, train_loss: 0.9547200202941895
step 9100, train_loss: 0.9535565972328186
step 9120, train_loss: 0.9547179937362671
step 9140, train_loss: 0.9553114175796509
step 9160, train_loss: 0.9538386464118958
step 9180, train_loss: 0.9545749425888062
step 9200, train_loss: 0.9533891677856445
step 9220, train_loss: 0.9548884630203247
step 9240, train_loss: 0.9539744257926941
step 9260, train_loss: 0.9553812742233276
step 9280, train_loss: 0.952564001083374
step 9300, train_loss: 0.9545693397521973
step 9320, train_loss: 0.953890860080719
step 9340, train_loss: 0.9521567821502686
step 9360, train_loss: 0.9532983303070068
step 9380, train_loss: 0.9524754285812378
step 9400, train_loss: 0.9547102451324463
step 9420, train_loss: 0.9533777236938477
step 9440, train_loss: 0.95402991771698
step 9460, train_loss: 0.9543460607528687
step 9480, train_loss: 0.9507718682289124
step 9500, train_loss: 0.9534106850624084
step 9520, train_loss: 0.9517579078674316
step 9540, train_loss: 0.9519577622413635
step 9560, train_loss: 0.9537023901939392
step 9580, train_loss: 0.9525036811828613
step 9600, train_loss: 0.9535046219825745
step 9620, train_loss: 0.9525538682937622
step 9640, train_loss: 0.9528281092643738
step 9660, train_loss: 0.9520205855369568
step 9680, train_loss: 0.9535301923751831
step 9700, train_loss: 0.953705370426178
step 9720, train_loss: 0.952845573425293
step 9740, train_loss: 0.9538934230804443
step 9760, train_loss: 0.9528218507766724
step 9780, train_loss: 0.9519343376159668
step 9800, train_loss: 0.9525995254516602
step 9820, train_loss: 0.9511880874633789
step 9840, train_loss: 0.9522857666015625
step 9860, train_loss: 0.9498578906059265
step 9880, train_loss: 0.9517879486083984
step 9900, train_loss: 0.9523724317550659
step 9920, train_loss: 0.9533379077911377
step 9940, train_loss: 0.9524639844894409
step 9960, train_loss: 0.9532973766326904
step 9980, train_loss: 0.9523241519927979
step 10000, train_loss: 0.9494159817695618
step 10020, train_loss: 0.952316164970398
step 10040, train_loss: 0.950638473033905
step 10060, train_loss: 0.9511435627937317
step 10080, train_loss: 0.9522721767425537
step 10100, train_loss: 0.952707052230835
step 10120, train_loss: 0.9500144720077515
step 10140, train_loss: 0.9500262141227722
step 10160, train_loss: 0.9480204582214355
step 10180, train_loss: 0.9511346817016602
step 10200, train_loss: 0.9515647292137146
step 10220, train_loss: 0.9492433071136475
step 10240, train_loss: 0.9518686532974243
step 10260, train_loss: 0.9517412185668945
step 10280, train_loss: 0.9522382020950317
step 10300, train_loss: 0.9498641490936279
step 10320, train_loss: 0.9505062103271484
step 10340, train_loss: 0.9508647918701172
step 10360, train_loss: 0.9500927925109863
step 10380, train_loss: 0.9497939348220825
step 10400, train_loss: 0.9500691890716553
step 10420, train_loss: 0.9512394070625305
step 10440, train_loss: 0.9517734050750732
step 10460, train_loss: 0.9481647610664368
step 10480, train_loss: 0.9494591951370239
step 10500, train_loss: 0.9503709673881531
step 10520, train_loss: 0.9494677782058716
step 10540, train_loss: 0.9478915333747864
step 10560, train_loss: 0.9476131796836853
step 10580, train_loss: 0.9504365921020508
step 10600, train_loss: 0.9504802227020264
step 10620, train_loss: 0.9462884664535522
step 10640, train_loss: 0.9466153383255005
step 10660, train_loss: 0.9492446184158325
step 10680, train_loss: 0.9478282928466797
step 10700, train_loss: 0.9486205577850342
step 10720, train_loss: 0.9474325776100159
step 10740, train_loss: 0.9494105577468872
step 10760, train_loss: 0.94908607006073
step 10780, train_loss: 0.9492703676223755
step 10800, train_loss: 0.9485054016113281
step 10820, train_loss: 0.9475972652435303
step 10840, train_loss: 0.9444161057472229
step 10860, train_loss: 0.9475702047348022
step 10880, train_loss: 0.9476816058158875
step 10900, train_loss: 0.946929395198822
step 10920, train_loss: 0.9481101036071777
step 10940, train_loss: 0.9477596879005432
step 10960, train_loss: 0.9492766261100769
step 10980, train_loss: 0.9460648894309998
step 11000, train_loss: 0.9469782114028931
step 11020, train_loss: 0.947417140007019
step 11040, train_loss: 0.9448872804641724
step 11060, train_loss: 0.9471938610076904
step 11080, train_loss: 0.945447564125061
step 11100, train_loss: 0.9470131993293762
step 11120, train_loss: 0.9466652870178223
step 11140, train_loss: 0.9461520314216614
step 11160, train_loss: 0.9466010928153992
step 11180, train_loss: 0.9454447031021118
step 11200, train_loss: 0.9463146924972534
step 11220, train_loss: 0.9440112113952637
step 11240, train_loss: 0.9467813372612
step 11260, train_loss: 0.9463537335395813
step 11280, train_loss: 0.9464173316955566
step 11300, train_loss: 0.9460828304290771
step 11320, train_loss: 0.9431877732276917
step 11340, train_loss: 0.9416123032569885
step 11360, train_loss: 0.9448275566101074
step 11380, train_loss: 0.942847490310669
step 11400, train_loss: 0.9428653120994568
step 11420, train_loss: 0.940818190574646
step 11440, train_loss: 0.9453703761100769
step 11460, train_loss: 0.942390501499176
step 11480, train_loss: 0.9439141750335693
step 11500, train_loss: 0.9405964612960815
step 11520, train_loss: 0.944336473941803
step 11540, train_loss: 0.9436458349227905
step 11560, train_loss: 0.9440592527389526
step 11580, train_loss: 0.9438660740852356
step 11600, train_loss: 0.9433976411819458
step 11620, train_loss: 0.9446338415145874
step 11640, train_loss: 0.9430350065231323
step 11660, train_loss: 0.9398118853569031
step 11680, train_loss: 0.940902590751648
step 11700, train_loss: 0.9390418529510498
step 11720, train_loss: 0.9420556426048279
step 11740, train_loss: 0.9429110288619995
step 11760, train_loss: 0.9410640001296997
step 11780, train_loss: 0.9432471990585327
step 11800, train_loss: 0.9398407340049744
step 11820, train_loss: 0.9409260749816895
step 11840, train_loss: 0.9425151348114014
step 11860, train_loss: 0.9403201341629028
step 11880, train_loss: 0.9423954486846924
step 11900, train_loss: 0.943579375743866
step 11920, train_loss: 0.9358359575271606
step 11940, train_loss: 0.9395847320556641
step 11960, train_loss: 0.9378148317337036
step 11980, train_loss: 0.9415932893753052
step 12000, train_loss: 0.942048192024231
step 12020, train_loss: 0.9407749176025391
step 12040, train_loss: 0.9394528865814209
step 12060, train_loss: 0.9413639903068542
step 12080, train_loss: 0.9400418400764465
step 12100, train_loss: 0.9402118921279907
step 12120, train_loss: 0.9373599886894226
step 12140, train_loss: 0.9396387338638306
step 12160, train_loss: 0.9385374784469604
step 12180, train_loss: 0.9402036666870117
step 12200, train_loss: 0.9405786991119385
step 12220, train_loss: 0.9402522444725037
step 12240, train_loss: 0.9407153725624084
step 12260, train_loss: 0.9380605220794678
step 12280, train_loss: 0.9385379552841187
step 12300, train_loss: 0.9394352436065674
step 12320, train_loss: 0.93995600938797
step 12340, train_loss: 0.9383611083030701
step 12360, train_loss: 0.9385838508605957
step 12380, train_loss: 0.9370955228805542
step 12400, train_loss: 0.9366857409477234
step 12420, train_loss: 0.9405622482299805
step 12440, train_loss: 0.940730631351471
step 12460, train_loss: 0.9372274875640869
step 12480, train_loss: 0.9380889534950256
step 12500, train_loss: 0.9390473365783691
step 12520, train_loss: 0.9364348649978638
step 12540, train_loss: 0.9363069534301758
step 12560, train_loss: 0.9352291226387024
step 12580, train_loss: 0.939369261264801
step 12600, train_loss: 0.9354161024093628
step 12620, train_loss: 0.9389941692352295
step 12640, train_loss: 0.9386142492294312
step 12660, train_loss: 0.9361116886138916
step 12680, train_loss: 0.9385490417480469
step 12700, train_loss: 0.9361616373062134
step 12720, train_loss: 0.9322093725204468
step 12740, train_loss: 0.9357322454452515
step 12760, train_loss: 0.937454104423523
step 12780, train_loss: 0.9367835521697998
step 12800, train_loss: 0.9378253221511841
step 12820, train_loss: 0.9335280060768127
step 12840, train_loss: 0.9354721307754517
step 12860, train_loss: 0.936080813407898
step 12880, train_loss: 0.9385308623313904
step 12900, train_loss: 0.9345179200172424
step 12920, train_loss: 0.9339371919631958
step 12940, train_loss: 0.9357635974884033
step 12960, train_loss: 0.9372174739837646
step 12980, train_loss: 0.9367987513542175
step 13000, train_loss: 0.9381774663925171
step 13020, train_loss: 0.9370574355125427
step 13040, train_loss: 0.9318989515304565
step 13060, train_loss: 0.9316691160202026
step 13080, train_loss: 0.9364429712295532
step 13100, train_loss: 0.9364502429962158
step 13120, train_loss: 0.9332923293113708
step 13140, train_loss: 0.9354828596115112
step 13160, train_loss: 0.9337490797042847
step 13180, train_loss: 0.9347076416015625
step 13200, train_loss: 0.9325003623962402
step 13220, train_loss: 0.9363341927528381
step 13240, train_loss: 0.9311556816101074
step 13260, train_loss: 0.9359320402145386
step 13280, train_loss: 0.9350704550743103
step 13300, train_loss: 0.9333186149597168
step 13320, train_loss: 0.9344576597213745
step 13340, train_loss: 0.9361563920974731
step 13360, train_loss: 0.9286662936210632
step 13380, train_loss: 0.9321231245994568
step 13400, train_loss: 0.9363999366760254
step 13420, train_loss: 0.9338150024414062
step 13440, train_loss: 0.9292806386947632
step 13460, train_loss: 0.9320610761642456
step 13480, train_loss: 0.9326200485229492
step 13500, train_loss: 0.9323197603225708
step 13520, train_loss: 0.9370541572570801
step 13540, train_loss: 0.9349794387817383
step 13560, train_loss: 0.9347678422927856
step 13580, train_loss: 0.9336007833480835
step 13600, train_loss: 0.9347244501113892
step 13620, train_loss: 0.9292179346084595
step 13640, train_loss: 0.930620551109314
step 13660, train_loss: 0.9349384307861328
step 13680, train_loss: 0.9331538677215576
step 13700, train_loss: 0.9333532452583313
step 13720, train_loss: 0.9308435916900635
step 13740, train_loss: 0.9360947608947754
step 13760, train_loss: 0.9320081472396851
step 13780, train_loss: 0.9350556135177612
step 13800, train_loss: 0.9350483417510986
step 13820, train_loss: 0.9346402883529663
step 13840, train_loss: 0.9282464385032654
step 13860, train_loss: 0.9320774078369141
step 13880, train_loss: 0.9303335547447205
step 13900, train_loss: 0.9297198057174683
step 13920, train_loss: 0.9349182844161987
step 13940, train_loss: 0.9302465915679932
step 13960, train_loss: 0.933376669883728
step 13980, train_loss: 0.9339398145675659
step 14000, train_loss: 0.935577392578125
step 14020, train_loss: 0.9282099008560181
step 14040, train_loss: 0.934958279132843
step 14060, train_loss: 0.9282312989234924
step 14080, train_loss: 0.9325595498085022
step 14100, train_loss: 0.9286030530929565
step 14120, train_loss: 0.9317991733551025
step 14140, train_loss: 0.9295451641082764
step 14160, train_loss: 0.9317513704299927
step 14180, train_loss: 0.9328877925872803
step 14200, train_loss: 0.9312728047370911
step 14220, train_loss: 0.9338439702987671
step 14240, train_loss: 0.9338542222976685
step 14260, train_loss: 0.9323411583900452
step 14280, train_loss: 0.9341280460357666
step 14300, train_loss: 0.9340243339538574
step 14320, train_loss: 0.9318112134933472
step 14340, train_loss: 0.9272952079772949
step 14360, train_loss: 0.9329841136932373
step 14380, train_loss: 0.9349892139434814
step 14400, train_loss: 0.9282984733581543
step 14420, train_loss: 0.9308024048805237
step 14440, train_loss: 0.9286320209503174
step 14460, train_loss: 0.9337986707687378
step 14480, train_loss: 0.9302382469177246
step 14500, train_loss: 0.9275953769683838
step 14520, train_loss: 0.9330937266349792
step 14540, train_loss: 0.9329049587249756
step 14560, train_loss: 0.9309782981872559
step 14580, train_loss: 0.9312387108802795
step 14600, train_loss: 0.9319113492965698
step 14620, train_loss: 0.9313229322433472
step 14640, train_loss: 0.9288825988769531
step 14660, train_loss: 0.9291853308677673
step 14680, train_loss: 0.9314203858375549
step 14700, train_loss: 0.9316048622131348
step 14720, train_loss: 0.9239698648452759
step 14740, train_loss: 0.9311137795448303
step 14760, train_loss: 0.9269775152206421
step 14780, train_loss: 0.9310935139656067
step 14800, train_loss: 0.9234497547149658
step 14820, train_loss: 0.9320478439331055
step 14840, train_loss: 0.9322594404220581
step 14860, train_loss: 0.9318462610244751
step 14880, train_loss: 0.9267466068267822
step 14900, train_loss: 0.930169403553009
step 14920, train_loss: 0.9291243553161621
step 14940, train_loss: 0.9326604604721069
step 14960, train_loss: 0.931023895740509
step 14980, train_loss: 0.9228983521461487
step 15000, train_loss: 0.9295012950897217
step 15020, train_loss: 0.9303243160247803
step 15040, train_loss: 0.9258149862289429
step 15060, train_loss: 0.9273507595062256
step 15080, train_loss: 0.925945520401001
step 15100, train_loss: 0.9314470887184143
step 15120, train_loss: 0.9289858937263489
step 15140, train_loss: 0.9283853769302368
step 15160, train_loss: 0.9245047569274902
step 15180, train_loss: 0.929061770439148
step 15200, train_loss: 0.9287813305854797
step 15220, train_loss: 0.9313350915908813
step 15240, train_loss: 0.9261875748634338
step 15260, train_loss: 0.9304359555244446
step 15280, train_loss: 0.926103949546814
step 15300, train_loss: 0.9231436252593994
step 15320, train_loss: 0.9264642000198364
step 15340, train_loss: 0.9297144412994385
step 15360, train_loss: 0.9293535947799683
step 15380, train_loss: 0.9265012145042419
step 15400, train_loss: 0.9234873652458191
step 15420, train_loss: 0.9279191493988037
step 15440, train_loss: 0.9303231835365295
step 15460, train_loss: 0.9303720593452454
step 15480, train_loss: 0.9243545532226562
step 15500, train_loss: 0.9265832901000977
step 15520, train_loss: 0.9301767349243164
step 15540, train_loss: 0.9255259037017822
step 15560, train_loss: 0.9294787049293518
step 15580, train_loss: 0.9291757345199585
step 15600, train_loss: 0.9280039668083191
step 15620, train_loss: 0.9279639720916748
step 15640, train_loss: 0.928236186504364
step 15660, train_loss: 0.9300458431243896
step 15680, train_loss: 0.9305115938186646
step 15700, train_loss: 0.9263026714324951
step 15720, train_loss: 0.9266879558563232
step 15740, train_loss: 0.9251945614814758
step 15760, train_loss: 0.9284854531288147
step 15780, train_loss: 0.9259241223335266
step 15800, train_loss: 0.9235650300979614
step 15820, train_loss: 0.9253047704696655
step 15840, train_loss: 0.924121618270874
step 15860, train_loss: 0.9263272285461426
step 15880, train_loss: 0.928534209728241
step 15900, train_loss: 0.9273282289505005
step 15920, train_loss: 0.9244108200073242
step 15940, train_loss: 0.9237556457519531
step 15960, train_loss: 0.9276870489120483
step 15980, train_loss: 0.9243488907814026
step 16000, train_loss: 0.9270083904266357
step 16020, train_loss: 0.9258251190185547
step 16040, train_loss: 0.9271422028541565
step 16060, train_loss: 0.9281953573226929
step 16080, train_loss: 0.9273020029067993
step 16100, train_loss: 0.9243184924125671
step 16120, train_loss: 0.9284250736236572
step 16140, train_loss: 0.9260116815567017
step 16160, train_loss: 0.9227660298347473
step 16180, train_loss: 0.926414966583252
step 16200, train_loss: 0.9247976541519165
step 16220, train_loss: 0.9238770604133606
step 16240, train_loss: 0.9225834012031555
step 16260, train_loss: 0.9227430820465088
step 16280, train_loss: 0.9280312061309814
step 16300, train_loss: 0.9198857545852661
step 16320, train_loss: 0.9212678670883179
step 16340, train_loss: 0.9209948778152466
step 16360, train_loss: 0.924301266670227
step 16380, train_loss: 0.9235851764678955
step 16400, train_loss: 0.9215116500854492
step 16420, train_loss: 0.9216733574867249
step 16440, train_loss: 0.9238675832748413
step 16460, train_loss: 0.9250562191009521
step 16480, train_loss: 0.9254716634750366
step 16500, train_loss: 0.9244388341903687
step 16520, train_loss: 0.9229075908660889
step 16540, train_loss: 0.9226000308990479
step 16560, train_loss: 0.922258734703064
step 16580, train_loss: 0.9197880029678345
step 16600, train_loss: 0.9226101636886597
step 16620, train_loss: 0.9219975471496582
step 16640, train_loss: 0.9242284297943115
step 16660, train_loss: 0.9184807538986206
step 16680, train_loss: 0.9230340719223022
step 16700, train_loss: 0.9152092933654785
step 16720, train_loss: 0.9220846891403198
step 16740, train_loss: 0.9195927977561951
step 16760, train_loss: 0.9232879877090454
step 16780, train_loss: 0.9187356233596802
step 16800, train_loss: 0.9219471216201782
step 16820, train_loss: 0.9205116033554077
step 16840, train_loss: 0.9215614795684814
step 16860, train_loss: 0.9213447570800781
step 16880, train_loss: 0.9172543883323669
step 16900, train_loss: 0.9138143658638
step 16920, train_loss: 0.911227822303772
step 16940, train_loss: 0.9227252006530762
step 16960, train_loss: 0.9156731367111206
step 16980, train_loss: 0.9214999675750732
step 17000, train_loss: 0.9130048751831055
step 17020, train_loss: 0.9186931848526001
step 17040, train_loss: 0.9150805473327637
step 17060, train_loss: 0.9227005243301392
step 17080, train_loss: 0.917332649230957
step 17100, train_loss: 0.9171106219291687
step 17120, train_loss: 0.9172452688217163
step 17140, train_loss: 0.9172879457473755
step 17160, train_loss: 0.9195833206176758
step 17180, train_loss: 0.9122006893157959
step 17200, train_loss: 0.9185779094696045
step 17220, train_loss: 0.9171233177185059
step 17240, train_loss: 0.9149083495140076
step 17260, train_loss: 0.9199205040931702
step 17280, train_loss: 0.917236328125
step 17300, train_loss: 0.918834924697876
step 17320, train_loss: 0.9154559373855591
step 17340, train_loss: 0.916705846786499
step 17360, train_loss: 0.9078830480575562
step 17380, train_loss: 0.9162582159042358
step 17400, train_loss: 0.9178283214569092
step 17420, train_loss: 0.9157477617263794
step 17440, train_loss: 0.918757438659668
step 17460, train_loss: 0.9169182181358337
step 17480, train_loss: 0.9086229801177979
step 17500, train_loss: 0.9164389371871948
step 17520, train_loss: 0.9127899408340454
step 17540, train_loss: 0.915786623954773
step 17560, train_loss: 0.9172034859657288
step 17580, train_loss: 0.9172431230545044
step 17600, train_loss: 0.915662944316864
step 17620, train_loss: 0.9087352156639099
step 17640, train_loss: 0.9095245003700256
step 17660, train_loss: 0.9124219417572021
step 17680, train_loss: 0.915090799331665
step 17700, train_loss: 0.9187140464782715
step 17720, train_loss: 0.9129157066345215
step 17740, train_loss: 0.9129984974861145
step 17760, train_loss: 0.9129138588905334
step 17780, train_loss: 0.915184497833252
step 17800, train_loss: 0.9101094007492065
step 17820, train_loss: 0.9077585935592651
step 17840, train_loss: 0.9143370389938354
step 17860, train_loss: 0.9064128398895264
step 17880, train_loss: 0.91263347864151
step 17900, train_loss: 0.9132900834083557
step 17920, train_loss: 0.9085180759429932
step 17940, train_loss: 0.910324215888977
step 17960, train_loss: 0.9051060676574707
step 17980, train_loss: 0.913097620010376
step 18000, train_loss: 0.9063681364059448
step 18020, train_loss: 0.9098944664001465
step 18040, train_loss: 0.9067944884300232
step 18060, train_loss: 0.9128233790397644
step 18080, train_loss: 0.9132099151611328
step 18100, train_loss: 0.9125308394432068
step 18120, train_loss: 0.9028459787368774
step 18140, train_loss: 0.9118056893348694
step 18160, train_loss: 0.9088501930236816
step 18180, train_loss: 0.9087783098220825
step 18200, train_loss: 0.9094530344009399
step 18220, train_loss: 0.9061383008956909
step 18240, train_loss: 0.9040030837059021
step 18260, train_loss: 0.9077049493789673
step 18280, train_loss: 0.9035388231277466
step 18300, train_loss: 0.9058635234832764
step 18320, train_loss: 0.9108317494392395
step 18340, train_loss: 0.9065563678741455
step 18360, train_loss: 0.9021382927894592
step 18380, train_loss: 0.9094464778900146
step 18400, train_loss: 0.9096695184707642
step 18420, train_loss: 0.9002923369407654
step 18440, train_loss: 0.909156322479248
step 18460, train_loss: 0.9075085520744324
step 18480, train_loss: 0.9088848829269409
step 18500, train_loss: 0.9095864295959473
step 18520, train_loss: 0.9088386297225952
step 18540, train_loss: 0.904995322227478
step 18560, train_loss: 0.9048171043395996
step 18580, train_loss: 0.9002335667610168
step 18600, train_loss: 0.9050099849700928
step 18620, train_loss: 0.9038496017456055
step 18640, train_loss: 0.9028391242027283
step 18660, train_loss: 0.9078108072280884
step 18680, train_loss: 0.9076914191246033
step 18700, train_loss: 0.9074974060058594
step 18720, train_loss: 0.9061702489852905
step 18740, train_loss: 0.9074704647064209
step 18760, train_loss: 0.9078344106674194
step 18780, train_loss: 0.8962638974189758
step 18800, train_loss: 0.9003528356552124
step 18820, train_loss: 0.9054014086723328
step 18840, train_loss: 0.9065245389938354
step 18860, train_loss: 0.8993452191352844
step 18880, train_loss: 0.9037679433822632
step 18900, train_loss: 0.8983404636383057
step 18920, train_loss: 0.9009379148483276
step 18940, train_loss: 0.9024015069007874
step 18960, train_loss: 0.9066371917724609
step 18980, train_loss: 0.8976817727088928
step 19000, train_loss: 0.9008896350860596
step 19020, train_loss: 0.904234766960144
step 19040, train_loss: 0.9083947539329529
step 19060, train_loss: 0.905160665512085
step 19080, train_loss: 0.905842661857605
step 19100, train_loss: 0.9024534225463867
step 19120, train_loss: 0.9029972553253174
step 19140, train_loss: 0.9057745337486267
step 19160, train_loss: 0.9029972553253174
step 19180, train_loss: 0.8958802819252014
step 19200, train_loss: 0.8979710340499878
step 19220, train_loss: 0.9059503078460693
step 19240, train_loss: 0.8999613523483276
step 19260, train_loss: 0.8994933366775513
step 19280, train_loss: 0.9051366448402405
step 19300, train_loss: 0.8997981548309326
step 19320, train_loss: 0.9055734872817993
step 19340, train_loss: 0.9005032181739807
step 19360, train_loss: 0.899113655090332
step 19380, train_loss: 0.9028170704841614
step 19400, train_loss: 0.8992838859558105
step 19420, train_loss: 0.8966460227966309
step 19440, train_loss: 0.903356671333313
step 19460, train_loss: 0.8969002366065979
step 19480, train_loss: 0.9009789228439331
step 19500, train_loss: 0.897557258605957
step 19520, train_loss: 0.8996397852897644
step 19540, train_loss: 0.8942302465438843
step 19560, train_loss: 0.9022752046585083
step 19580, train_loss: 0.9008330702781677
step 19600, train_loss: 0.8971235752105713
step 19620, train_loss: 0.8968083262443542
step 19640, train_loss: 0.9003320932388306
step 19660, train_loss: 0.9016289710998535
step 19680, train_loss: 0.8987922668457031
step 19700, train_loss: 0.90010666847229
step 19720, train_loss: 0.8995134830474854
step 19740, train_loss: 0.8935803174972534
step 19760, train_loss: 0.8990165591239929
step 19780, train_loss: 0.8944333791732788
step 19800, train_loss: 0.9009855389595032
step 19820, train_loss: 0.9008526802062988
step 19840, train_loss: 0.8960686922073364
step 19860, train_loss: 0.8948673605918884
step 19880, train_loss: 0.8929142951965332
step 19900, train_loss: 0.900779128074646
step 19920, train_loss: 0.8949199914932251
step 19940, train_loss: 0.8994263410568237
step 19960, train_loss: 0.8957359194755554
step 19980, train_loss: 0.889930248260498
step 20000, train_loss: 0.8977998495101929
step 20020, train_loss: 0.8930721282958984
step 20040, train_loss: 0.8943709135055542
step 20060, train_loss: 0.8998898863792419
step 20080, train_loss: 0.8943095207214355
step 20100, train_loss: 0.8941607475280762
step 20120, train_loss: 0.8933439254760742
step 20140, train_loss: 0.8957836031913757
step 20160, train_loss: 0.8915926218032837
step 20180, train_loss: 0.8873258233070374
step 20200, train_loss: 0.898140549659729
step 20220, train_loss: 0.8946666121482849
step 20240, train_loss: 0.8970049619674683
step 20260, train_loss: 0.8943134546279907
step 20280, train_loss: 0.8941736221313477
step 20300, train_loss: 0.8957110047340393
step 20320, train_loss: 0.894819974899292
step 20340, train_loss: 0.8944592475891113
step 20360, train_loss: 0.895250141620636
step 20380, train_loss: 0.8933830261230469
step 20400, train_loss: 0.8966304063796997
step 20420, train_loss: 0.8950384855270386
step 20440, train_loss: 0.8900846838951111
step 20460, train_loss: 0.8979506492614746
step 20480, train_loss: 0.8958906531333923
step 20500, train_loss: 0.8978011608123779
step 20520, train_loss: 0.8947350382804871
step 20540, train_loss: 0.8901171088218689
step 20560, train_loss: 0.8896374702453613
step 20580, train_loss: 0.8954218626022339
step 20600, train_loss: 0.8910134434700012
step 20620, train_loss: 0.8855732083320618
step 20640, train_loss: 0.8956059217453003
step 20660, train_loss: 0.8902339935302734
step 20680, train_loss: 0.8953734636306763
step 20700, train_loss: 0.8951879739761353
step 20720, train_loss: 0.8871532082557678
step 20740, train_loss: 0.8936957120895386
step 20760, train_loss: 0.892626166343689
step 20780, train_loss: 0.8903902173042297
step 20800, train_loss: 0.8869249820709229
step 20820, train_loss: 0.8918961882591248
step 20840, train_loss: 0.8877193927764893
step 20860, train_loss: 0.890108048915863
step 20880, train_loss: 0.8843225240707397
step 20900, train_loss: 0.8938104510307312
step 20920, train_loss: 0.8905848860740662
step 20940, train_loss: 0.8911631107330322
step 20960, train_loss: 0.8899728059768677
step 20980, train_loss: 0.8920178413391113
step 21000, train_loss: 0.8789942264556885
step 21020, train_loss: 0.8810027241706848
step 21040, train_loss: 0.8892450332641602
step 21060, train_loss: 0.8873038291931152
step 21080, train_loss: 0.8873685598373413
step 21100, train_loss: 0.8865542411804199
step 21120, train_loss: 0.8840608596801758
step 21140, train_loss: 0.8827176094055176
step 21160, train_loss: 0.8844528198242188
step 21180, train_loss: 0.8886094689369202
step 21200, train_loss: 0.8823110461235046
step 21220, train_loss: 0.8883373737335205
step 21240, train_loss: 0.8878517746925354
step 21260, train_loss: 0.8838548064231873
step 21280, train_loss: 0.8879805207252502
step 21300, train_loss: 0.8867669105529785
step 21320, train_loss: 0.8828908205032349
step 21340, train_loss: 0.8879629373550415
step 21360, train_loss: 0.8838834762573242
step 21380, train_loss: 0.8779140710830688
step 21400, train_loss: 0.8831378221511841
step 21420, train_loss: 0.8871980905532837
step 21440, train_loss: 0.8831844925880432
step 21460, train_loss: 0.8810551166534424
step 21480, train_loss: 0.8845812082290649
step 21500, train_loss: 0.8883166909217834
step 21520, train_loss: 0.8859394788742065
step 21540, train_loss: 0.8856492042541504
step 21560, train_loss: 0.8865455389022827
step 21580, train_loss: 0.8780667185783386
step 21600, train_loss: 0.8871421813964844
step 21620, train_loss: 0.884419322013855
step 21640, train_loss: 0.8815242648124695
step 21660, train_loss: 0.8766647577285767
step 21680, train_loss: 0.8808556795120239
step 21700, train_loss: 0.8778248429298401
step 21720, train_loss: 0.8866838812828064
step 21740, train_loss: 0.886269211769104
step 21760, train_loss: 0.8798972964286804
step 21780, train_loss: 0.8820185661315918
step 21800, train_loss: 0.8807048797607422
step 21820, train_loss: 0.8790912628173828
step 21840, train_loss: 0.8814957737922668
step 21860, train_loss: 0.8742516040802002
step 21880, train_loss: 0.8857632279396057
step 21900, train_loss: 0.8855959177017212
step 21920, train_loss: 0.8840179443359375
step 21940, train_loss: 0.8856184482574463
step 21960, train_loss: 0.8734642863273621
step 21980, train_loss: 0.8830011487007141
step 22000, train_loss: 0.8767347931861877
step 22020, train_loss: 0.8769949674606323
step 22040, train_loss: 0.8775429129600525
step 22060, train_loss: 0.8830156922340393
step 22080, train_loss: 0.871515691280365
step 22100, train_loss: 0.8824902772903442
step 22120, train_loss: 0.8782883882522583
step 22140, train_loss: 0.8836173415184021
step 22160, train_loss: 0.8777809143066406
step 22180, train_loss: 0.8766207695007324
step 22200, train_loss: 0.8811104893684387
step 22220, train_loss: 0.8818538188934326
step 22240, train_loss: 0.8752762079238892
step 22260, train_loss: 0.8785977959632874
step 22280, train_loss: 0.8748173117637634
step 22300, train_loss: 0.879051148891449
step 22320, train_loss: 0.878552258014679
step 22340, train_loss: 0.8694150447845459
step 22360, train_loss: 0.8812047243118286
step 22380, train_loss: 0.8768419027328491
step 22400, train_loss: 0.8735883831977844
step 22420, train_loss: 0.8707071542739868
step 22440, train_loss: 0.8789223432540894
step 22460, train_loss: 0.8717617988586426
step 22480, train_loss: 0.8820475339889526
step 22500, train_loss: 0.8753479719161987
step 22520, train_loss: 0.8797388076782227
step 22540, train_loss: 0.8747106194496155
step 22560, train_loss: 0.8773198127746582
step 22580, train_loss: 0.8779639601707458
step 22600, train_loss: 0.8785426020622253
step 22620, train_loss: 0.8763375878334045
step 22640, train_loss: 0.8628115653991699
step 22660, train_loss: 0.8766584992408752
step 22680, train_loss: 0.877842128276825
step 22700, train_loss: 0.8732779622077942
step 22720, train_loss: 0.8700621128082275
step 22740, train_loss: 0.8754780292510986
step 22760, train_loss: 0.8724020719528198
step 22780, train_loss: 0.8777786493301392
step 22800, train_loss: 0.8696479797363281
step 22820, train_loss: 0.8705370426177979
step 22840, train_loss: 0.8675669431686401
step 22860, train_loss: 0.8785313963890076
step 22880, train_loss: 0.8717629909515381
step 22900, train_loss: 0.8706501722335815
step 22920, train_loss: 0.8723739981651306
step 22940, train_loss: 0.8665770292282104
step 22960, train_loss: 0.8735838532447815
step 22980, train_loss: 0.8703200817108154
step 23000, train_loss: 0.8740756511688232
step 23020, train_loss: 0.8784812688827515
step 23040, train_loss: 0.8732943534851074
step 23060, train_loss: 0.8640278577804565
step 23080, train_loss: 0.8591562509536743
step 23100, train_loss: 0.8671237230300903
step 23120, train_loss: 0.8619645833969116
step 23140, train_loss: 0.8680779337882996
step 23160, train_loss: 0.873365044593811
step 23180, train_loss: 0.8748897314071655
step 23200, train_loss: 0.864396870136261
step 23220, train_loss: 0.8765882253646851
step 23240, train_loss: 0.8736331462860107
step 23260, train_loss: 0.8759431838989258
step 23280, train_loss: 0.8708969950675964
step 23300, train_loss: 0.8730646967887878
step 23320, train_loss: 0.8662582635879517
step 23340, train_loss: 0.8655909895896912
step 23360, train_loss: 0.8690077066421509
step 23380, train_loss: 0.8722617626190186
step 23400, train_loss: 0.866078794002533
step 23420, train_loss: 0.865084171295166
step 23440, train_loss: 0.8731725811958313
step 23460, train_loss: 0.8616690039634705
step 23480, train_loss: 0.8664674758911133
step 23500, train_loss: 0.8667440414428711
step 23520, train_loss: 0.8729413151741028
step 23540, train_loss: 0.8649253845214844
step 23560, train_loss: 0.8686928153038025
step 23580, train_loss: 0.870904803276062
step 23600, train_loss: 0.8717121481895447
step 23620, train_loss: 0.8695002198219299
step 23640, train_loss: 0.8671239614486694
step 23660, train_loss: 0.8638712167739868
step 23680, train_loss: 0.8764665722846985
step 23700, train_loss: 0.8625478744506836
step 23720, train_loss: 0.8619718551635742
step 23740, train_loss: 0.8667889833450317
step 23760, train_loss: 0.8735222220420837
step 23780, train_loss: 0.8679836392402649
step 23800, train_loss: 0.8629501461982727
step 23820, train_loss: 0.8658997416496277
step 23840, train_loss: 0.8675245046615601
step 23860, train_loss: 0.8675684928894043
step 23880, train_loss: 0.8675397634506226
step 23900, train_loss: 0.8701348304748535
step 23920, train_loss: 0.8646674156188965
step 23940, train_loss: 0.8680757284164429
step 23960, train_loss: 0.8708176612854004
step 23980, train_loss: 0.867957592010498
step 24000, train_loss: 0.8658240437507629
step 24020, train_loss: 0.8609883785247803
step 24040, train_loss: 0.8627939224243164
step 24060, train_loss: 0.8724063634872437
step 24080, train_loss: 0.8669596314430237
step 24100, train_loss: 0.8671524524688721
step 24120, train_loss: 0.8595193028450012
step 24140, train_loss: 0.8690744042396545
step 24160, train_loss: 0.8644044399261475
step 24180, train_loss: 0.8592913150787354
step 24200, train_loss: 0.8655927181243896
step 24220, train_loss: 0.8613948822021484
step 24240, train_loss: 0.8693410754203796
step 24260, train_loss: 0.8706754446029663
step 24280, train_loss: 0.8596464395523071
step 24300, train_loss: 0.8646770715713501
step 24320, train_loss: 0.866726279258728
step 24340, train_loss: 0.8560234904289246
step 24360, train_loss: 0.8704593181610107
step 24380, train_loss: 0.8632824420928955
step 24400, train_loss: 0.8592582941055298
step 24420, train_loss: 0.8617404699325562
step 24440, train_loss: 0.8646955490112305
step 24460, train_loss: 0.8654179573059082
step 24480, train_loss: 0.863502025604248
step 24500, train_loss: 0.8657777309417725
step 24520, train_loss: 0.8691011071205139
step 24540, train_loss: 0.8544526696205139
step 24560, train_loss: 0.8676029443740845
step 24580, train_loss: 0.8641901612281799
step 24600, train_loss: 0.8657971620559692
step 24620, train_loss: 0.8636115789413452
step 24640, train_loss: 0.8669863939285278
step 24660, train_loss: 0.8635649681091309
step 24680, train_loss: 0.868059515953064
step 24700, train_loss: 0.8649024963378906
step 24720, train_loss: 0.8635299801826477
step 24740, train_loss: 0.8606915473937988
step 24760, train_loss: 0.850542426109314
step 24780, train_loss: 0.8635354042053223
step 24800, train_loss: 0.8581134080886841
step 24820, train_loss: 0.8640809059143066
step 24840, train_loss: 0.8522355556488037
step 24860, train_loss: 0.8590338826179504
step 24880, train_loss: 0.8609097599983215
step 24900, train_loss: 0.8532213568687439
step 24920, train_loss: 0.8612437844276428
step 24940, train_loss: 0.8628494739532471
step 24960, train_loss: 0.8518651127815247
step 24980, train_loss: 0.8582152128219604
step 25000, train_loss: 0.8562664985656738
step 25020, train_loss: 0.8571888208389282
step 25040, train_loss: 0.8508105278015137
step 25060, train_loss: 0.8532677888870239
step 25080, train_loss: 0.8571005463600159
step 25100, train_loss: 0.8592928647994995
step 25120, train_loss: 0.8622978925704956
step 25140, train_loss: 0.8656215667724609
step 25160, train_loss: 0.8537856340408325
step 25180, train_loss: 0.8581266403198242
step 25200, train_loss: 0.8648867607116699
step 25220, train_loss: 0.863553524017334
step 25240, train_loss: 0.8610295057296753
step 25260, train_loss: 0.8545440435409546
step 25280, train_loss: 0.8597638010978699
step 25300, train_loss: 0.8655437231063843
step 25320, train_loss: 0.8579390048980713
step 25340, train_loss: 0.8576194047927856
step 25360, train_loss: 0.8536851406097412
step 25380, train_loss: 0.860304057598114
step 25400, train_loss: 0.8596731424331665
step 25420, train_loss: 0.85542893409729
step 25440, train_loss: 0.8541837930679321
step 25460, train_loss: 0.8542781472206116
step 25480, train_loss: 0.8521516919136047
step 25500, train_loss: 0.8594526052474976
step 25520, train_loss: 0.8570346236228943
step 25540, train_loss: 0.8596656322479248
step 25560, train_loss: 0.8618948459625244
step 25580, train_loss: 0.8562341928482056
step 25600, train_loss: 0.8624483346939087
step 25620, train_loss: 0.8640491962432861
step 25640, train_loss: 0.8594666719436646
step 25660, train_loss: 0.858565092086792
step 25680, train_loss: 0.8546972870826721
step 25700, train_loss: 0.8580398559570312
step 25720, train_loss: 0.8528915643692017
step 25740, train_loss: 0.8608453273773193
step 25760, train_loss: 0.8606011867523193
step 25780, train_loss: 0.8543808460235596
step 25800, train_loss: 0.8573789000511169
step 25820, train_loss: 0.8544204235076904
step 25840, train_loss: 0.847607433795929
step 25860, train_loss: 0.8533830046653748
step 25880, train_loss: 0.8611956238746643
step 25900, train_loss: 0.8580260872840881
step 25920, train_loss: 0.8509997129440308
step 25940, train_loss: 0.8575714826583862
step 25960, train_loss: 0.8628772497177124
step 25980, train_loss: 0.862445056438446
step 26000, train_loss: 0.8643478155136108
step 26020, train_loss: 0.8601105809211731
step 26040, train_loss: 0.8585753440856934
step 26060, train_loss: 0.8572267293930054
step 26080, train_loss: 0.8537177443504333
step 26100, train_loss: 0.8518091440200806
step 26120, train_loss: 0.855697751045227
step 26140, train_loss: 0.8505667448043823
step 26160, train_loss: 0.8599854707717896
step 26180, train_loss: 0.8586825132369995
step 26200, train_loss: 0.8537260293960571
step 26220, train_loss: 0.850697934627533
step 26240, train_loss: 0.856154203414917
step 26260, train_loss: 0.8583695888519287
step 26280, train_loss: 0.850459635257721
step 26300, train_loss: 0.8546233177185059
step 26320, train_loss: 0.849730372428894
step 26340, train_loss: 0.8568349480628967
step 26360, train_loss: 0.8544139266014099
step 26380, train_loss: 0.8496840000152588
step 26400, train_loss: 0.8619067668914795
step 26420, train_loss: 0.8545246124267578
step 26440, train_loss: 0.85223388671875
step 26460, train_loss: 0.8431999683380127
step 26480, train_loss: 0.8465654253959656
step 26500, train_loss: 0.8538005352020264
step 26520, train_loss: 0.8541539311408997
step 26540, train_loss: 0.8447299003601074
step 26560, train_loss: 0.851243257522583
step 26580, train_loss: 0.8496569991111755
step 26600, train_loss: 0.849341630935669
step 26620, train_loss: 0.842802882194519
step 26640, train_loss: 0.8467565774917603
step 26660, train_loss: 0.8591006994247437
step 26680, train_loss: 0.852856457233429
step 26700, train_loss: 0.8529925346374512
step 26720, train_loss: 0.850715160369873
step 26740, train_loss: 0.8468274474143982
step 26760, train_loss: 0.8583703637123108
step 26780, train_loss: 0.8449306488037109
step 26800, train_loss: 0.8557797074317932
step 26820, train_loss: 0.8447974920272827
step 26840, train_loss: 0.8560200929641724
step 26860, train_loss: 0.8495707511901855
step 26880, train_loss: 0.8503322601318359
step 26900, train_loss: 0.8523754477500916
step 26920, train_loss: 0.8522987961769104
step 26940, train_loss: 0.8449674844741821
step 26960, train_loss: 0.847111701965332
step 26980, train_loss: 0.8473758101463318
step 27000, train_loss: 0.8534388542175293
step 27020, train_loss: 0.8492035269737244
step 27040, train_loss: 0.8503563404083252
step 27060, train_loss: 0.855135440826416
step 27080, train_loss: 0.8466497659683228
step 27100, train_loss: 0.8489230275154114
step 27120, train_loss: 0.8533340692520142
step 27140, train_loss: 0.8500770926475525
step 27160, train_loss: 0.8456692099571228
step 27180, train_loss: 0.845909833908081
step 27200, train_loss: 0.8478164672851562
step 27220, train_loss: 0.847754716873169
step 27240, train_loss: 0.8403093814849854
step 27260, train_loss: 0.8446223735809326
step 27280, train_loss: 0.8480086326599121
step 27300, train_loss: 0.847256600856781
step 27320, train_loss: 0.843374490737915
step 27340, train_loss: 0.8479601740837097
step 27360, train_loss: 0.8462342023849487
step 27380, train_loss: 0.84763503074646
step 27400, train_loss: 0.8419404029846191
step 27420, train_loss: 0.8500039577484131
step 27440, train_loss: 0.8407628536224365
step 27460, train_loss: 0.8472661972045898
step 27480, train_loss: 0.8464564085006714
step 27500, train_loss: 0.8369849324226379
step 27520, train_loss: 0.8528568744659424
step 27540, train_loss: 0.8442062139511108
step 27560, train_loss: 0.8503208756446838
step 27580, train_loss: 0.8440939784049988
step 27600, train_loss: 0.8382478952407837
step 27620, train_loss: 0.8490428924560547
step 27640, train_loss: 0.8526036739349365
step 27660, train_loss: 0.8471859693527222
step 27680, train_loss: 0.8457683324813843
step 27700, train_loss: 0.851265549659729
step 27720, train_loss: 0.8529542684555054
step 27740, train_loss: 0.8440582752227783
step 27760, train_loss: 0.843656063079834
step 27780, train_loss: 0.8415021300315857
step 27800, train_loss: 0.8389856815338135
step 27820, train_loss: 0.851216733455658
step 27840, train_loss: 0.8402141332626343
step 27860, train_loss: 0.8389295339584351
step 27880, train_loss: 0.8351204991340637
step 27900, train_loss: 0.8414058089256287
step 27920, train_loss: 0.8519592881202698
step 27940, train_loss: 0.834762454032898
step 27960, train_loss: 0.8498121500015259
step 27980, train_loss: 0.8513820171356201
step 28000, train_loss: 0.8482762575149536
step 28020, train_loss: 0.839450478553772
step 28040, train_loss: 0.8456763625144958
step 28060, train_loss: 0.8500730991363525
step 28080, train_loss: 0.8398113250732422
step 28100, train_loss: 0.8496854305267334
step 28120, train_loss: 0.8397110104560852
step 28140, train_loss: 0.8457024693489075
step 28160, train_loss: 0.8406728506088257
step 28180, train_loss: 0.8530582785606384
step 28200, train_loss: 0.8464228510856628
step 28220, train_loss: 0.8482949137687683
step 28240, train_loss: 0.8403695821762085
step 28260, train_loss: 0.838791012763977
step 28280, train_loss: 0.8379970788955688
step 28300, train_loss: 0.8462901711463928
step 28320, train_loss: 0.8425317406654358
step 28340, train_loss: 0.8434351682662964
step 28360, train_loss: 0.8447788953781128
step 28380, train_loss: 0.8421303629875183
step 28400, train_loss: 0.8456636667251587
step 28420, train_loss: 0.8399503827095032
step 28440, train_loss: 0.8466106653213501
step 28460, train_loss: 0.8462430238723755
step 28480, train_loss: 0.8441749811172485
step 28500, train_loss: 0.8472506999969482
step 28520, train_loss: 0.8371790647506714
step 28540, train_loss: 0.8471734523773193
step 28560, train_loss: 0.8416244983673096
step 28580, train_loss: 0.8405674695968628
step 28600, train_loss: 0.8477088212966919
step 28620, train_loss: 0.8436787128448486
step 28640, train_loss: 0.8372324705123901
step 28660, train_loss: 0.8354557752609253
step 28680, train_loss: 0.8416576385498047
step 28700, train_loss: 0.8384537696838379
step 28720, train_loss: 0.8462711572647095
step 28740, train_loss: 0.8335375189781189
step 28760, train_loss: 0.8411734104156494
step 28780, train_loss: 0.8372935652732849
step 28800, train_loss: 0.8425012826919556
step 28820, train_loss: 0.8424098491668701
step 28840, train_loss: 0.8418835401535034
step 28860, train_loss: 0.8355593681335449
step 28880, train_loss: 0.841541051864624
step 28900, train_loss: 0.8309310674667358
step 28920, train_loss: 0.8365514874458313
step 28940, train_loss: 0.8473703861236572
step 28960, train_loss: 0.8404508233070374
step 28980, train_loss: 0.84488844871521
step 29000, train_loss: 0.843369722366333
step 29020, train_loss: 0.8384015560150146
step 29040, train_loss: 0.8456015586853027
step 29060, train_loss: 0.8399993181228638
step 29080, train_loss: 0.8420687913894653
step 29100, train_loss: 0.8355149030685425
step 29120, train_loss: 0.8421398401260376
step 29140, train_loss: 0.8375398516654968
step 29160, train_loss: 0.8308771848678589
step 29180, train_loss: 0.8384520411491394
step 29200, train_loss: 0.8438411951065063
step 29220, train_loss: 0.8342058658599854
step 29240, train_loss: 0.8396381139755249
step 29260, train_loss: 0.8422015905380249
step 29280, train_loss: 0.8416284918785095
step 29300, train_loss: 0.8398364782333374
step 29320, train_loss: 0.8408206701278687
step 29340, train_loss: 0.8304098844528198
step 29360, train_loss: 0.8379130363464355
step 29380, train_loss: 0.8375731706619263
step 29400, train_loss: 0.8415691256523132
step 29420, train_loss: 0.8393765687942505
step 29440, train_loss: 0.8441053032875061
step 29460, train_loss: 0.8348560333251953
step 29480, train_loss: 0.8448874950408936
step 29500, train_loss: 0.8384767174720764
step 29520, train_loss: 0.8359856009483337
step 29540, train_loss: 0.8402743339538574
step 29560, train_loss: 0.8415851593017578
step 29580, train_loss: 0.8430256247520447
step 29600, train_loss: 0.8423177003860474
step 29620, train_loss: 0.8303986191749573
step 29640, train_loss: 0.8389171957969666
step 29660, train_loss: 0.8385149240493774
step 29680, train_loss: 0.8392219543457031
step 29700, train_loss: 0.8457821011543274
step 29720, train_loss: 0.8373463749885559
step 29740, train_loss: 0.8387777805328369
step 29760, train_loss: 0.8362467885017395
step 29780, train_loss: 0.840314507484436
step 29800, train_loss: 0.833815336227417
step 29820, train_loss: 0.8399425745010376
step 29840, train_loss: 0.8379335403442383
step 29860, train_loss: 0.8355603814125061
step 29880, train_loss: 0.8412609100341797
step 29900, train_loss: 0.8372050523757935
step 29920, train_loss: 0.839465856552124
step 29940, train_loss: 0.8370383381843567
step 29960, train_loss: 0.8387043476104736
step 29980, train_loss: 0.8399856090545654
OCDBT is initialized successfully.
Saving item to exps/blues_driver/1/Gaussian_blues_driver/ckpt.
Renaming exps/blues_driver/1/Gaussian_blues_driver/ckpt.orbax-checkpoint-tmp-1702982562796314 to exps/blues_driver/1/Gaussian_blues_driver/ckpt
Finished saving checkpoint to `exps/blues_driver/1/Gaussian_blues_driver/ckpt`.
